\section{Предобработка текста и извлечение признаков}

\subsection{Необходимость предобработки текста}

Входные данные для задач на естественном языке состоят из необработанного неструктурированного текста. Текстовая информация, в отличие от других типов данных, таких как изображения или временные ряды, не обладает числовым представлением, поэтому перед подачей ее в какой-либо классификатор она должна быть спроецирована в соответствующее пространство признаков. Поэтому процедуры предварительной обработки имеют особое значение, поскольку без них не существует основы ни для процедур выделения признаков, ни для алгоритмов классификации.

\subsection{Очистка и предобработка текста}
\subsubsection{Токенизация}
Токенизация --- самая базовая операция предварительной обработки, которую необходимо применить к тексту. Этот процесс определяет уровень детализации анализа текстовых данных и в целом может быть описан как процесс предварительной обработки, целью которого является разделение текстового потока на слова, фразы, символы или другие значимые элементы, называемые токенами. Разделение основано на правилах и может быть простым, как разделение пробелами или знаками препинания.

Например, предложение:

After eating, I decided to start working.

В данном случае токены следующие:

\{“After”, “eating”, “I”, “decided”, “to”, “start”, “working”\}.

\subsubsection{Удаление шума (стоп-слов)}
Стоп-слова (текстовые шумы) —-- это неинформативные слова, которые встречаются в большом количестве, но не имеют семантического значения. Например, слова ``и'', ``в'', ``только'' не несут никакой ценности и только добавляют шум в данные. Множество токенов, полученный после процесса токенизации, может содержать множество ненужных или бессмысленных элементов. Удаление стоп-слов необходимо, поскольку это уменьшит количество различных элементов в пространстве признаков. 

Обычно тексты содержат разные грамматические формы одного и того же слова, а также могут встречаться однокоренные слова. Лемматизация и стемминг преследуют цель привести все встречающиеся словоформы к одной, нормальной словарной форме. 

Стемминг –-- это грубый эвристический процесс, который отрезает ``лишнее'' от корня слов, часто это приводит к потере словообразовательных суффиксов. 

Лемматизация –-- это более тонкий процесс, который использует словарь и морфологический анализ, чтобы в итоге привести слово к его канонической форме –-- лемме. 

Отличие в том, что стеммер (конкретная реализация алгоритма стемминга) действует без учёта контекста и, соответственно, не делает разницы между словами, которые имеют разный смысл в зависимости от части речи. Однако у стеммеров есть своё преимущество –-- они работают быстрее.

\subsection{Извлечения признаков}
\subsubsection{Bag of Words (BoW)}
Модель Bag of Words (модель BoW) —-- это уменьшенное и упрощенное представление текстового документа из выбранных частей текста на основе определенных критериев, таких как частота слов.

При всей простоте реализации данный подход имеет ряд недостатков:
\begin{itemize}[label = ---]
    \item для больших наборов текстов размерность словаря, а, следовательно, и размерность вектора, представляющего текст, может исчисляться сотнями тысяч, а иногда и миллионами;
    \item не учитывается контекст слова в документе.
\end{itemize}

%Таким образом, модели BoW обычно реализуются в сочетании с методами извлечения признаков, основанными на множественности слов, что позволяет поддерживать один вектор для каждого документа, а не один для каждого слова. Для этой цели частота термина (Term Frequency --- TF) подсчитывает, сколько раз слово встречается в тексте. Применительно к корпусу текстов, а не к явному подсчету, обычной практикой является использование относительной частоты термина в тексте по отношению к другим документам. 

%Можно также заметить, что в очень больших корпусах общие слова, в частности, по своей сути менее полезны (поскольку они появляются во всех документах и не помогают их различать). Следовательно, TF часто взвешивается по обратной частоте документов (Inverse Document Frequency --- IDF), что уменьшает влияние общих слов за счет снижения их общей оценки (и повышения оценки более редких слов).
\subsubsection{Word2Vec}
Word2Vec (Word to Vector) —-- это метод, используемый для преобразования слов в векторы, тем самым фиксируя их значение, семантическое сходство и взаимосвязь с окружающим текстом. Этот метод помогает компьютерам изучать контекст и значение выражений и ключевых слов из больших текстовых коллекций, таких как новостные статьи и книги.

Основная идея Word2Vec состоит в том, чтобы представить каждое слово как многомерный вектор, где положение вектора в этом многомерном пространстве отражает значение слова.

Word2Vec использует модель мелкой нейронной сети для изучения значения слов из большого массива текстов. В отличие от глубоких нейронных сетей, которые имеют несколько скрытых слоев, мелкие нейронные сети имеют только один или два скрытых слоя между входом и выходом. Это делает обработку быстрой и прозрачной. Неглубокая нейронная сеть Word2Vec может быстро распознавать семантические сходства и идентифицировать слова-синонимы, что делает ее быстрее глубоких нейронных сетей.
\subsubsection{GloVe}
GloVe (Global Vector)  --- алгоритм обучения без учителя  для получения векторных представлений слов. Обучение проводится на основе агрегированной глобальной статистики частоты совпадения слов из корпуса, и полученные представления демонстрируют интересные линейные подструктуры векторного пространства слов. Преимущества GloVe:
\begin{itemize}[label = ---]
    \item Простая архитектура без нейронной сети.
    \item Модель быстрая, и этого может быть достаточно для простых приложений.
    \item GloVe улучшает Word2Vec. Она добавляет частоту встречаемости слов и опережает Word2Vec в большинстве приложений.
    \item Осмысленные эмбеддинги.
\end{itemize}
Недостатки алгоритма:
\begin{itemize}[label = ---]
    \item Хотя матрица совместной встречаемости предоставляет глобальную информацию, GloVe остаётся обученной на уровне слов и даёт немного данных о предложении и контексте, в котором слово используется.
    \item Плохо обрабатывает неизвестные и редкие слова.
\end{itemize}

\subsubsection{FastText}
FastText —-- это cозданная в Facebook библиотека, содержащая предобученные готовые векторные представления слов и классификатор, то есть алгоритм машинного обучения разбивающий тексты на классы.

К основной модели Word2Vec добавлена модель символьных n-грамм. Каждое слово представляется композицией нескольких последовательностей символов определённой длины. Например, слово they в зависимости от гиперпараметров, может состоять из “th”, “he”, “ey”, “the”, “hey”. По сути, вектор слова – это сумма всех его n-грамм.

Результаты работы классификатора хорошо подходят для слов с небольшой частотой встречаемости, так как они разделяются на n-граммы. В отличие от Word2Vec и Glove, модель способна генерировать эмбеддинги для неизвестных слов.