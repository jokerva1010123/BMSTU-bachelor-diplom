\section{Существующие методы классификации текста}
\subsection{KNN}
Метод k-ближайших соседей (k-nearest neighbors) –-- это простой алгоритм машинного обучения с учителем, который можно использовать для решения задач классификации и регрессии.

Алгоритм K-NN сохраняет все доступные данные и классифицирует новую точку данных на основе сходства. Это означает, что когда появляются новые данные, их можно легко классифицировать по категории наборов с помощью алгоритма K-NN.

Согласно принципу алгоритма KNN, структура классификатора включает в себя 4 параметра: данные для классификации, набор выборочных данных, набор выборочных меток и значение K. Затем вычислить расстояние между новыми данными и выборочными данными, упорядочить расстояния от наименьшего к наибольшему, возьмите первые K ближайших данных. Наиболее часто встречающаяся метка может быть идентифицирована как новая метка данных путем определения количества вхождений каждого введенного типа данных в К первых точках.

Преимущества метода.
\begin{itemize}
	\item[$-$] Алгоритм прост и легко реализуем.
	\item[$-$] Нет необходимости строить модель, настраивать несколько параметров или делать дополнительные допущения.
	\item[$-$] Алгоритм универсален. Его можно использовать для обоих типов задач: классификации и регрессии.	
\end{itemize}
Недостатки метода.
\begin{itemize}
	\item[$-$] Алгоритм работает значительно медленнее при увеличении объема выборки, предикторов или независимых переменных.
	\item[$-$] Из аргумента выше следуют большие вычислительные затраты во время выполнения.
	\item[$-$] Всегда нужно определять оптимальное значение k.
\end{itemize}
\subsection{Метод опорных векторов}
Метод опорных векторов (англ. support vector machine, SVM) — один из наиболее популярных методов обучения, который применяется для решения задач классификации и регрессии. Основная идея метода заключается в построении гиперплоскости, разделяющей объекты выборки оптимальным способом.

\begin{itemize}
\item[$-$] Преимущества метода:
\begin{itemize}
	\item[$+$] хорошо работает с пространством признаков большего размера;
	\item[$+$] хорошо работает с данными небольшего объема;
	\item[$+$] метод находит разделяющую полосу максимальной ширины, позволяет в дальнейшем осуществлять более уверенную классификацию.
\end{itemize}
\item[$-$] Недостатки метода:
\begin{itemize}
	\item[$+$] долгое время обучения (для больших наборов данных);
	\item[$+$] неустойчивость к шуму: выбросы в исходных данных становятся опорными объектами-нарушителями и напрямую влияют на построение разделяющей гиперплоскости.
\end{itemize}
\end{itemize}
\subsection{Decision Tree and Random Forest}
Деревья решений (Decision Tree)\cite{45} являются одними из самых ранних и популярных классификаторов. Структура этого метода представляет собой иерархическую декомпозицию пространства данных\cite{7}, \cite{161}. Основная идея заключается в создании дерева на основе атрибута для категоризированных точек данных, но основная задача дерева решений заключается в том, какой атрибут или функция может находиться на родительском уровне, а какой должен быть на дочернем уровне.

Дерево решений —-- это очень быстрый алгоритм как для обучения, так и для прогнозирования, но он также чрезвычайно чувствителен к небольшим изменениям в данных\cite{166}. Эта модель также имеет проблемы с прогнозированием вне выборки\cite{168}.

Метод случайных лесов (Random Forest) --— это метод обучения для классификации текста. Случайные леса представляют собой наборы деревьев решений, обученных с использованием случайных подмножеств признаков, которые достигли гораздо более высокой производительности и чаще используются на практике. Этот метод очень быстро обучается работе с наборами текстовых данных по сравнению с другими методами, такими как глубокое обучение, но довольно медленным для создания прогнозов после обучения\cite{172}. Таким образом, чтобы добиться более быстрой структуры, количество деревьев в лесу необходимо уменьшить, поскольку большее количество деревьев в лесу увеличивает временную сложность на этапе прогнозирования.
\subsection{CNN}
Сверточная нейронная сеть (CNN) —-- это архитектура глубокого обучения, которая обычно используется для иерархической классификации документов \cite{6, 168}. Хотя CNN изначально были созданы для обработки изображений, они также эффективно использовались для классификации текста\cite{27, 187}.

В базовой CNN для обработки изображений тензор изображения свернут с набором ядер размера d × d. Эти слои свертки называются картами объектов и могут объединяться для предоставления нескольких входных фильтров. Чтобы снизить сложность вычислений, CNN используют пуллинг для уменьшения размера выходных данных от одного уровня сети к другому. Различные методы объединения используются для уменьшения выходных данных при сохранении важных функций\cite{188}. Чтобы передать объединенные выходные данные составных избранных карт на следующий слой, карты сводятся в один столбец. Последние слои CNN обычно полностью связаны. В общем, на этапе обратного распространения ошибки сверточной нейронной сети корректируются как веса, так и фильтры детектора признаков. Потенциальная проблема, которая возникает при использовании CNN для классификации текста, это количество «каналов» S (размер пространства признаков). Хотя приложения классификации изображений обычно имеют мало каналов (например, только 3 канала RGB), S может быть очень большим (например, 50000) для приложений классификации текста, что приводит к очень высокой размерности. 

Было предложено множество подходов, одним из самых популярных является TextCNN\cite{69}, сравнительно простая модель на основе CNN с однослойной структурой свертки, которая размещается поверх вложений слов.

